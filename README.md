# ğŸš€ Modern Data Pipeline Project

A comprehensive real-time data pipeline implementation following modern data engineering practices. This project demonstrates Change Data Capture (CDC) from MySQL, streaming through Kafka, processing with PySpark, and storing in MongoDB/Redis.

## ğŸ“‹ Table of Contents

- [Project Structure](#project-structure)
- [Architecture Overview](#architecture-overview)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Quick Start with Docker](#quick-start-with-docker)
- [Configuration](#configuration)
- [Key Components](#key-components)
- [Development Workflow](#development-workflow)
- [Production Deployment](#production-deployment)
- [Monitoring & Observability](#monitoring--observability)
- [Testing Strategy](#testing-strategy)
- [Contributing](#contributing)

## ğŸ“ Project Structure

```
data-pipeline-project/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ database.yaml
â”‚   â”œâ”€â”€ kafka.yaml
â”‚   â””â”€â”€ spark.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mysql_connector.py
â”‚   â”‚   â”œâ”€â”€ mongodb_connector.py
â”‚   â”‚   â”œâ”€â”€ redis_connector.py
â”‚   â”‚   â””â”€â”€ kafka_connector.py
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mysql_schemas.py
â”‚   â”‚   â”œâ”€â”€ kafka_schemas.py
â”‚   â”‚   â””â”€â”€ validation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ triggers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database_trigger.py
â”‚   â”‚   â”œâ”€â”€ cdc_handler.py
â”‚   â”‚   â””â”€â”€ scheduler.py
â”‚   â”‚
â”‚   â”œâ”€â”€ producers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”‚   â””â”€â”€ data_publisher.py
â”‚   â”‚
â”‚   â”œâ”€â”€ consumers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kafka_consumer.py
â”‚   â”‚   â””â”€â”€ spark_streaming.py
â”‚   â”‚
â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py
â”‚   â”‚   â”œâ”€â”€ data_validator.py
â”‚   â”‚   â”œâ”€â”€ business_logic.py
â”‚   â”‚   â””â”€â”€ spark_transformations.py
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mongodb_writer.py
â”‚   â”‚   â”œâ”€â”€ redis_cache.py
â”‚   â”‚   â””â”€â”€ batch_writer.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â”œâ”€â”€ helpers.py
â”‚       â””â”€â”€ exceptions.py
â”‚
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ realtime_pipeline.py
â”‚   â”œâ”€â”€ batch_pipeline.py
â”‚   â””â”€â”€ data_quality_check.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_connectors.py
â”‚   â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”‚   â””â”€â”€ test_validators.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â”‚   â””â”€â”€ test_kafka_flow.py
â”‚   â””â”€â”€ fixtures/
â”‚       â”œâ”€â”€ sample_data.json
â”‚       â””â”€â”€ test_config.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_databases.py
â”‚   â”œâ”€â”€ create_kafka_topics.py
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â””â”€â”€ monitoring.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_exploration.ipynb
â”‚   â”œâ”€â”€ pipeline_testing.ipynb
â”‚   â””â”€â”€ performance_analysis.ipynb
â”‚
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ helm/
â”‚   â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”‚   â”œâ”€â”€ values.yaml
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â””â”€â”€ rules.yaml
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â””â”€â”€ alerts/
â”‚       â””â”€â”€ pipeline_alerts.yaml
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ setup_guide.md
    â”œâ”€â”€ api_documentation.md
    â””â”€â”€ troubleshooting.md
```

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MySQL  â”‚â”€â”€â”€â–¶â”‚ Trigger â”‚â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â–¶â”‚  Spark  â”‚â”€â”€â”€â–¶â”‚MongoDB/ â”‚
â”‚ (Source)â”‚    â”‚  (CDC)  â”‚    â”‚(Stream) â”‚    â”‚(Process)â”‚    â”‚ Redis   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow:**
1. **MySQL CDC**: Monitor binlog Ä‘á»ƒ capture changes (INSERT/UPDATE/DELETE)
2. **Kafka Streaming**: Publish events vá»›i data validation
3. **Spark Processing**: Real-time transformation vÃ  business logic
4. **Storage**: Write vÃ o MongoDB vÃ  cache Redis

## âœ¨ Features

- ğŸ”„ **Real-time Change Data Capture** tá»« MySQL binlog
- ğŸ“¨ **Event-driven Architecture** vá»›i Apache Kafka
- âš¡ **Stream Processing** vá»›i PySpark Structured Streaming
- ğŸ—„ï¸ **Multi-storage Support** (MongoDB, Redis caching)
- ğŸ” **Data Validation** vá»›i Pydantic schemas
- ğŸ“Š **Observability** vá»›i Prometheus/Grafana
- ğŸ³ **Containerized Deployment** vá»›i Docker/Kubernetes
- ğŸ§ª **Comprehensive Testing** (unit, integration, performance)
- ğŸ“ˆ **Auto-scaling** vÃ  fault tolerance

## ğŸ› ï¸ Prerequisites

- **Docker** vÃ  **Docker Compose**
- **Python 3.9+**
- **Java 8/11** (cho Apache Spark)
- **Git**

## ğŸš€ Quick Start with Docker

### 1. Clone Repository
```bash
git clone <repository-url>
cd data-pipeline-project
```

### 2. Environment Setup
```bash
# Copy environment template
cp .env.example .env

# Chá»‰nh sá»­a cáº¥u hÃ¬nh theo mÃ´i trÆ°á»ng
nano .env
```

### 3. Start Infrastructure vá»›i Docker Compose
```bash
# Start táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra services Ä‘ang cháº¡y
docker-compose ps

# Xem logs
docker-compose logs -f
```

### 4. Initialize Database & Kafka Topics
```bash
# Setup MySQL schemas
docker-compose exec pipeline python scripts/setup_databases.py

# Táº¡o Kafka topics
docker-compose exec pipeline python scripts/create_kafka_topics.py
```

### 5. Run Data Pipeline
```bash
# Cháº¡y pipeline
docker-compose exec pipeline python scripts/run_pipeline.py

# Hoáº·c cháº¡y realtime job
docker-compose exec pipeline python jobs/realtime_pipeline.py
```

### 6. Access Services
- **Pipeline API**: http://localhost:8080
- **Kafka UI**: http://localhost:8081
- **Spark UI**: http://localhost:4040
- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090

## âš™ï¸ Configuration

### Docker Environment Variables (.env)
```bash
# Database Configuration
MYSQL_HOST=mysql
MYSQL_PORT=3306
MYSQL_USER=pipeline_user
MYSQL_PASSWORD=secure_password
MYSQL_DATABASE=source_db

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC_PREFIX=data_pipeline

# MongoDB Configuration
MONGODB_URI=mongodb://mongodb:27017
MONGODB_DATABASE=processed_data

# Redis Configuration
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# Spark Configuration
SPARK_MASTER=local[*]
SPARK_APP_NAME=DataPipeline

# Monitoring
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
```

### Configuration Files
- `config/database.yaml` - Database connection settings
- `config/kafka.yaml` - Kafka topics vÃ  consumer groups
- `config/spark.yaml` - Spark job configurations

## ğŸ”§ Key Components

### 1. Config Management
- **Centralized configuration** vá»›i YAML files
- **Environment-specific settings**
- **Database connection parameters** vá»›i retry logic

### 2. Connectors
- **Modular database connectors** (MySQL, MongoDB, Redis)
- **Kafka integration** cho messaging
- **Connection pooling** vÃ  retry mechanisms

### 3. Data Flow Components

#### Triggers & CDC (Change Data Capture)
```python
# src/triggers/cdc_handler.py
class MySQLCDCHandler:
    def setup_binlog_listener(self):
        # Monitor MySQL binlog for changes
        pass
    
    def handle_change_event(self, event):
        # Process INSERT/UPDATE/DELETE events
        pass
```

#### Kafka Integration
```python
# src/producers/kafka_producer.py
class DataProducer:
    def publish_change_event(self, topic, data):
        # Send data to Kafka topic
        pass
```

#### Spark Processing
```python
# src/consumers/spark_streaming.py
class SparkStreamProcessor:
    def process_kafka_stream(self):
        # Real-time data processing
        pass
    
    def write_to_storage(self, df):
        # Write to MongoDB/Redis
        pass
```

### 4. Modern Features

#### Data Validation
- **Pydantic models** cho data validation
- **Schema evolution handling**
- **Data quality checks** tá»± Ä‘á»™ng

#### Observability
- **Prometheus metrics** collection
- **Structured logging** vá»›i correlation IDs
- **Distributed tracing** cho debugging

#### Containerization
- **Docker containers** cho táº¥t cáº£ components
- **Kubernetes deployment** manifests
- **Helm charts** cho easy deployment

## ğŸ› ï¸ Development Workflow

### Local Development
```bash
# Setup virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start services vá»›i Docker Compose
docker-compose up -d

# Run pipeline locally
python scripts/run_pipeline.py
```

### Docker Development
```bash
# Build custom image
docker build -t data-pipeline:dev .

# Run vá»›i development compose
docker-compose -f docker-compose.dev.yml up -d

# Attach to container Ä‘á»ƒ debug
docker-compose exec pipeline bash
```

### Testing Strategy
- **Unit tests** cho individual components
- **Integration tests** cho end-to-end flow
- **Performance testing** vá»›i sample data

```bash
# Run tests trong container
docker-compose exec pipeline pytest tests/

# Run vá»›i coverage
docker-compose exec pipeline pytest --cov=src tests/
```

## ğŸš¢ Production Deployment

### Docker Production
```bash
# Build production image
docker build -f Dockerfile.prod -t data-pipeline:prod .

# Deploy vá»›i production compose
docker-compose -f docker-compose.prod.yml up -d
```

### Kubernetes Deployment
```bash
# Apply Kubernetes manifests
kubectl apply -f deployment/kubernetes/

# Sá»­ dá»¥ng Helm
helm install data-pipeline deployment/helm/

# Check pods
kubectl get pods -n data-pipeline
```

### Scalability Considerations
- **Horizontal scaling** vá»›i Kubernetes HPA
- **Kafka partitioning** cho parallel processing
- **Redis clustering** cho high availability
- **Load balancing** cho API endpoints

### Reliability Features
- **Error handling** vÃ  retry mechanisms
- **Dead letter queues** cho failed messages
- **Health checks** vÃ  monitoring
- **Circuit breakers** cho external dependencies

### Security
- **Secret management** vá»›i Kubernetes secrets
- **Network policies** vÃ  encryption
- **Access control** vÃ  authentication
- **Container security** scanning

## ğŸ“Š Monitoring & Observability

### Metrics Dashboard
- **Pipeline throughput** (messages/second)
- **Processing latency** (p95, p99)
- **Error rates** vÃ  failed messages
- **Resource utilization** (CPU, memory)
- **Data quality scores**

### Alerting
```bash
# Configure alerts
kubectl apply -f monitoring/alerts/pipeline_alerts.yaml

# View Prometheus rules
kubectl apply -f monitoring/prometheus/rules.yaml
```

### Log Aggregation
- **Structured logging** vá»›i JSON format
- **Correlation IDs** cho request tracing
- **Log levels** configuration
- **Centralized logging** vá»›i ELK stack

## ğŸ§ª Testing Strategy

### Test Categories
1. **Unit Tests**: Individual component testing
2. **Integration Tests**: End-to-end pipeline testing
3. **Performance Tests**: Load vÃ  stress testing
4. **Data Quality Tests**: Schema validation vÃ  data integrity

### Running Tests
```bash
# All tests
docker-compose exec pipeline pytest

# Specific categories
docker-compose exec pipeline pytest tests/unit/
docker-compose exec pipeline pytest tests/integration/

# Performance tests
docker-compose exec pipeline pytest tests/performance/ -v
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Make changes vÃ  add tests
4. Run tests: `docker-compose exec pipeline pytest`
5. Commit changes (`git commit -m 'Add amazing feature'`)
6. Push to branch (`git push origin feature/amazing-feature`)
7. Create Pull Request

### Development Guidelines
- Follow **PEP 8** style guide
- Write **comprehensive tests**
- Update **documentation**
- Add **type hints**
- Include **docstrings**

## ğŸ“„ Architecture Benefits

This structure follows modern data engineering best practices:

- âœ… **Microservices Architecture**: Modular, scalable components
- âœ… **Event-Driven Design**: Loosely coupled, reactive systems
- âœ… **Infrastructure as Code**: Reproducible deployments
- âœ… **Observability-First**: Comprehensive monitoring
- âœ… **Cloud-Native**: Container-ready, Kubernetes-native
- âœ… **DevOps Integration**: CI/CD pipeline ready
- âœ… **Data Quality**: Built-in validation vÃ  monitoring
- âœ… **Fault Tolerance**: Resilient error handling

## ğŸ†˜ Troubleshooting

### Common Issues

#### Services Not Starting
```bash
# Check Docker status
docker-compose ps

# View logs
docker-compose logs <service-name>

# Restart services
docker-compose restart
```

#### Data Not Flowing
```bash
# Check Kafka topics
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Monitor pipeline
docker-compose exec pipeline python scripts/monitoring.py
```

### Support
- ğŸ“– **Documentation**: [docs/](docs/)
- ğŸ› **Issues**: GitHub Issues
- ğŸ’¬ **Discussions**: GitHub Discussions

---

**Built with â¤ï¸ using modern data engineering practices**
